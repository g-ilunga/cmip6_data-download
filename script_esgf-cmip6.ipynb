{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<center>Downloading CMIP6 data from the ESGF platform using ESGF Python package \"esgf-pyclient\"</center>**\n",
    "\n",
    "*More info about the esgf-pyclient package is found here: https://esgf-pyclient.readthedocs.io/en/latest/index.html*\n",
    "\n",
    "*This script was developed on Python 3.9.19.*\n",
    "\n",
    "*I recommend that you create a virtual environment specifically for this script in which you will install the following packages: esgf-pyclient, pandas, xarray, requests*\n",
    "\n",
    "*Feel free to optimise this script and share the update*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Importing necessary libraries**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyesgf.search import SearchConnection\n",
    "import os\n",
    "import pandas as pd\n",
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Data search based on criteria**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we are defining criteria to be taken into account to obtain the desired dataset. \n",
    "\n",
    "project = [\"CMIP6\"] # This is to indicate that we want CMIP6 data\n",
    "\n",
    "source_id = [\"TaiESM1\"] # Here is to specify the models you are searching for. The list can contain more than one model.\n",
    "\n",
    "experiment_id = [\"historical\", \"ssp245\", \"ssp585\"] # Here is to specify if you want historical data or projection (ssp245, ssp585, etc.)\n",
    "\n",
    "variant_label = [\"r1i1p1f1\"] # Here is to specify the variant label\n",
    "\n",
    "frequency = [\"day\"] # Here is to specify the frequency\n",
    "\n",
    "variable = [\"tas\", \"tasmin\", \"tasmax\"] # Here is to specify the climate variable you are searching for\n",
    "\n",
    "# The list below (\"facets) must contain all the variables you intend to use to filter your search. \n",
    "# The element \"latest\" is to specify whether or not you want the latest data\n",
    "# The element \"replica\" is to specify whether or not you want duplicate to be included in your search results \n",
    "# It is advised to always keep \"latest\" and \"replica\" in the list\n",
    "facets = [\"project\", \"source_id\", \"experiment_id\", \"variable\", \"frequency\", \"variant_label\", \"latest\", \"replica\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we specify the node from which we want the data search to start. Setting the \"distrib\" parameter to \"true\" will expand the data search to other nodes\n",
    "conn = SearchConnection('https://esgf-node.ipsl.upmc.fr/esg-search', distrib = True)\n",
    "\n",
    "# Here we launch the search\n",
    "query = conn.new_context (latest = True, # Set to \"true\" to only get the latest version of dataset\n",
    "                          replica = False, # Set to \"false\" to avoid getting duplicate in the search result.\n",
    "                          project = project,\n",
    "                          source_id = source_id,\n",
    "                       experiment_id = experiment_id,\n",
    "                       variable = variable,\n",
    "                       frequency = frequency,\n",
    "                       variant_label = variant_label,\n",
    "                       facets = facets) # This is to confirm the search criteria we have set. Must always be kept here\n",
    "\n",
    "\n",
    "# Here is to obtain the total number of results the search has returned\n",
    "results_count = query.hit_count \n",
    "\n",
    "print (f\"The search has returned {results_count} results\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*It is important to highlight that each result contains a certain number of files. And each file is associated with a unique URL that can be used to download it. The line below is written to extract these URLs and store them in an Excel spreadsheet.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialising an empty list that will be used to store the extracted URLs\n",
    "urls = [] \n",
    "\n",
    "\n",
    "# Starting the extraction of URLs.\n",
    "for i in range(results_count): # This first loop will iterate over each result\n",
    "\n",
    "    dataset = query.search()[i] # This open a dataset \n",
    "\n",
    "    files_list = dataset.file_context().search() # This create a list of files contained in the opened dataset\n",
    "\n",
    "    for file in files_list: # This loop will iterate over each file of the list to extract their URLs\n",
    "\n",
    "        urls.append(file.download_url)\n",
    "\n",
    "    print (f\"Results {i+1} out of {results_count} processed\")\n",
    "\n",
    "# Saving the URLs in an Excel spreadsheet\n",
    "df = pd.DataFrame(urls, columns = [\"Links\"])\n",
    "\n",
    "df.to_excel(\"C:/Users/gilunga/Documents/files_url.xlsx\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*This script does not allow to filter date from the search results. If you want to remove some years, you can directly remove them from the created spreadsheet before heading to the next part of the script*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. Data download**\n",
    "\n",
    "*Using the URLs extracted in the previous step, we will now download the dataset of interest*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Opening the Excel spreadsheet containing the URLs\n",
    "df_urls = pd.read_excel(\"C:/Users/gilunga/Documents/files_url.xlsx\")\n",
    "\n",
    "# Converting the dataframe into list\n",
    "list_urls = df_urls[\"Links\"].tolist()\n",
    "\n",
    "# Directory where files will be saved\n",
    "path = \"C:/Users/gilunga/Documents/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Launching download\n",
    "for url in list_urls:\n",
    "\n",
    "    file_name = str(url.rsplit(\"/\", 1)[-1]) # Extracting the file name from the URL\n",
    "\n",
    "    output = path + file_name\n",
    "\n",
    "    response = requests.get(url, stream=True)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        \n",
    "        with open(output, 'wb') as f:\n",
    "            \n",
    "            for chunk in response.iter_content(chunk_size = 8192):\n",
    "                \n",
    "                f.write(chunk)\n",
    "\n",
    "        print(f'Successful download of: {file_name}')\n",
    "    \n",
    "    else:\n",
    "        print(f'Unsuccessful donwnload of: {file_name}. Status code: {response.status_code}')\n",
    "\n",
    "print (\"End of download. Check the log to ensure all data were successfuly downloaded\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cmip6_download",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
